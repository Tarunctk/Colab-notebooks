# -*- coding: utf-8 -*-
"""AI/ML/DS-Project:2023

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ofvQF64rUyq4PwDW85mbs_lYX9ArHz4t

# IMPORTING THE REQUIRED LIBRARIES
"""

#importing the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

import warnings
warnings.filterwarnings('ignore')

"""# UPLOADING THE DATASET"""

#read the dataset using pandas
bankdata=pd.read_csv("/content/drive/MyDrive/Outlook/bank-direct-marketing-campaigns.csv")

#creating a backup file for bank data set
bankdata_bk=bankdata.copy()

bankdata.head()

#displaying the dataset information
bankdata.info()

#display the columns with object data types
obj=bankdata.columns[bankdata.dtypes==object]
obj

#bankdata['job'].value_counts()
for i in obj:
  print(i,":")
  print(bankdata[i].value_counts())
  print()

bankdata.describe()

#shape of the dataset
bankdata.shape

"""# DATA PREPROCESSING"""

#checcking the duplicates in bank data set
bankdata_dup=bankdata[bankdata.duplicated(keep='last')]
bankdata_dup

#drop the duplicated rows in the given bank data set
bankdata=bankdata.drop_duplicates()

#reset the index
bankdata.reset_index(inplace=True)

#now again checking the shape of the data set
bankdata.shape

#checking the null values in data set
bankdata.isnull().sum()

# Filling the missing values using mode imputation method

# traindata2=traindata2.fillna(0) filling with custom value
# traindata2=traindata2.fillna(traindata2.mean())
# traindata2=traindata2.fillna(traindata2.mode().iloc[0])
# traindata2=traindata2.fillna(method='bfill')
# traindata2=traindata2.interpolate(method='linear')
#traindata2=traindata2.fillna(method='ffill')

#replacing the column names
bankdata.rename(columns=lambda x: x.replace('.', '_'), inplace=True)

bankdata.columns

#checking the outliers
#y_UL=round(bankdata.y.mean()+3*bankdata.y.std(),3)
#y_LL=round(bankdata.y.mean()-3*bankdata.y.std(),3)
#bankdata_new=bankdata[(bankdata.y>y_LL)&(bankdata.y<y_UL)]
#bankdata11_bk=bankdata_new
#bankdata_new.shape

"""# DATA TYPE CONVERSION"""

bankdata.info()

#counting the number of values in a column
bankdata['y'].value_counts()

#importing the label encoder
#label encoder is used to convert the data type from string to int
from sklearn.preprocessing import LabelEncoder
LE=LabelEncoder()
bankdata['y']=LE.fit_transform(bankdata['y'])

bankdata.head()

#checking the proportion of labels in target variable
y_count=bankdata.y.value_counts()
print("class 0:",y_count[0])
print("class 1:",y_count[1])
print("proportion:",round(y_count[0]/y_count[1],2),":1")
print("total records in bank marketing dataset:",len( bankdata))

#converting the data types using labelencoder
list1=['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']
for i in list1:
  bankdata[i]=LE.fit_transform(bankdata[i])

bankdata.info()

var=bankdata.columns[bankdata.dtypes==float]

for i in var:
  bankdata[i] = bankdata[i].astype(int)

bankdata.info()

#defining independent and dependent varialble
#indep_var=[]
#for col in bankdata.columns:
# if i!='y':
#  indep_var.append(col)
#target_var='y'
#x=bankdata[indep_var]
#y=bankdata[target_var]

x=bankdata.iloc[:,:-1]
y=bankdata.iloc[:,-1]
y

#splitting the dataset
from sklearn.model_selection import train_test_split as tts
x_train,x_test,y_train,y_test=tts(x,y,test_size=0.2,random_state=23)
x_train.shape,x_test.shape,y_train.shape,y_test.shape

#scling the features by using minmax scaler
from sklearn.preprocessing import MinMaxScaler
mmscaler=MinMaxScaler(feature_range=(0,1))
x_train=mmscaler.fit_transform(x_train)
x_train=pd.DataFrame(x_train)
x_test=mmscaler.fit_transform(x_test)
x_test=pd.DataFrame(x_test)

"""# KNN ALGORITHM"""

#reading the empty csv file for storing the result of KNN model
KNN_Results = pd.read_csv("/content/drive/MyDrive/Outlook/KNN_Results.csv")
KNN_Results.head()

# Build KNN Model

from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import sklearn.metrics as metrics

from sklearn.metrics import roc_curve, roc_auc_score

accuracy = []

for a in range(1, 21, 1):

    k = a

    # Build the model

    ModelKNN = KNeighborsClassifier(n_neighbors=k)

    # Train the model

    ModelKNN.fit(x_train, y_train)

    # Predict the model

    y_pred = ModelKNN.predict(x_test)
    y_pred_prob = ModelKNN.predict_proba(x_test)

    print('KNN_K_value = ', a)

    # Print the model name

    print('Model Name: ', ModelKNN)

    # confusion matrix in sklearn

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import classification_report

    # actual values

    actual = y_test

    # predicted values

    predicted = y_pred

    # confusion matrix

    matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)
    print('Confusion matrix : \n', matrix)

    # outcome values order in sklearn

    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)
    print('Outcome values : \n', tp, fn, fp, tn)

    # classification report for precision, recall f1-score and accuracy

    C_Report = classification_report(actual,predicted,labels=[1,0])

    print('Classification report : \n', C_Report)

    # calculating the metrics

    sensitivity = round(tp/(tp+fn), 3);
    specificity = round(tn/(tn+fp), 3);
    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);
    balanced_accuracy = round((sensitivity+specificity)/2, 3);

    precision = round(tp/(tp+fp), 3);
    f1Score = round((2*tp/(2*tp + fp + fn)), 3);

    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1.
    # A model with a score of +1 is a perfect model and -1 is a poor model

    from math import sqrt

    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)
    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)

    print('Accuracy :', round(accuracy*100, 2),'%')
    print('Precision :', round(precision*100, 2),'%')
    print('Recall :', round(sensitivity*100,2), '%')
    print('F1 Score :', f1Score)
    print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )
    print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')
    print('MCC :', MCC)

    # Area under ROC curve

    from sklearn.metrics import roc_curve, roc_auc_score

    print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))

    # ROC Curve

    from sklearn.metrics import roc_auc_score
    from sklearn.metrics import roc_curve
    model_roc_auc = roc_auc_score(actual, predicted)
    fpr, tpr, thresholds = roc_curve(actual, ModelKNN.predict_proba(x_test)[:,1])
    plt.figure()
    # plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
    plt.plot(fpr, tpr, label= 'Classification Model' % model_roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    #plt.savefig('Log_ROC')
    plt.show()
    #------------------------------------------------------------------------------
    new_row = {'Model Name' : ModelKNN,
               'KNN K Value' : a,
               'True_Positive' : tp,
               'False_Negative' : fn,
               'False_Positive' : fp,
               'True_Negative' : tn,
               'Accuracy' : accuracy,
               'Precision' : precision,
               'Recall' : sensitivity,
               'F1 Score' : f1Score,
               'Specificity' : specificity,
               'MCC':MCC,
               'ROC_AUC_Score':roc_auc_score(actual, predicted),
               'Balanced Accuracy':balanced_accuracy}
    KNN_Results = KNN_Results.append(new_row, ignore_index=True)
    #------KNN_Results------------------------------------------------------------------------

KNN_Results

"""# SVM Models"""

# Load the EMResults1 dataset for comparing SVM Algorithms

EMResults1 = pd.read_csv("/content/drive/MyDrive/Outlook/EMResults.csv")
EMResults1.head()

from sklearn.svm import SVC
#linear kernal
ModelSVM1 = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True,
                probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False,
                max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None)
#polynomial kernal
ModelSVMPoly = SVC(kernel='poly', degree=2, probability=True)
#guassian kernal
ModelSVMGaussian = SVC(kernel='rbf', random_state = 42, class_weight='balanced', probability=True)
#sigmoid kernal
ModelSVMSig = SVC(kernel='sigmoid', random_state = 42, class_weight='balanced', probability=True)
svcm=[ModelSVM1,ModelSVMPoly,ModelSVMGaussian,ModelSVMSig]

for models in svcm:

    # Train the model training dataset

    models.fit(x_train, y_train)

    # Prediction the model with test dataset

    y_pred = models.predict(x_test)
    y_pred_prob = models.predict_proba(x_test)

    # Print the model name

    print('Model Name: ', models)

    # confusion matrix in sklearn

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import classification_report

    # actual values

    actual = y_test

    # predicted values

    predicted = y_pred

    # confusion matrix

    matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)
    print('Confusion matrix : \n', matrix)

    # outcome values order in sklearn

    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)
    print('Outcome values : \n', tp, fn, fp, tn)

    # classification report for precision, recall f1-score and accuracy

    C_Report = classification_report(actual,predicted,labels=[1,0])

    print('Classification report : \n', C_Report)

    # calculating the metrics

    sensitivity = round(tp/(tp+fn), 3);
    specificity = round(tn/(tn+fp), 3);
    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);
    balanced_accuracy = round((sensitivity+specificity)/2, 3);

    precision = round(tp/(tp+fp), 3);
    f1Score = round((2*tp/(2*tp + fp + fn)), 3);

    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1.
    # A model with a score of +1 is a perfect model and -1 is a poor model

    from math import sqrt

    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)
    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)

    print('Accuracy :', round(accuracy*100, 2),'%')
    print('Precision :', round(precision*100, 2),'%')
    print('Recall :', round(sensitivity*100,2), '%')
    print('F1 Score :', f1Score)
    print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )
    print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')
    print('MCC :', MCC)

    # Area under ROC curve

    from sklearn.metrics import roc_curve, roc_auc_score

    print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))

    # ROC Curve

    from sklearn.metrics import roc_auc_score
    from sklearn.metrics import roc_curve
    Model_roc_auc = roc_auc_score(actual, predicted)
    fpr, tpr, thresholds = roc_curve(actual, models.predict_proba(x_test)[:,1])
    plt.figure()
    #
    plt.plot(fpr, tpr, label= 'Classification Model' % Model_roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.savefig('Log_ROC')
    plt.show()
    print('-----------------------------------------------------------------------------------------------------')
    #----------------------------------------------------------------------------------------------------------
    new_row = {'Model Name' : models,
               'True_Positive': tp,
               'False_Negative': fn,
               'False_Positive': fp,
               'True_Negative': tn,
               'Accuracy' : accuracy,
               'Precision' : precision,
               'Recall' : sensitivity,
               'F1 Score' : f1Score,
               'Specificity' : specificity,
               'MCC':MCC,
               'ROC_AUC_Score':roc_auc_score(actual, predicted),
               'Balanced Accuracy':balanced_accuracy}
    EMResults1 = EMResults1.append(new_row, ignore_index=True)
    #----------------------------------------------------------------------------------------------------------
#======================================================================================================================>

# Display EMResults1 records
EMResults1.head()

"""# Building the model"""

EM_Result=pd.read_csv("/content/drive/MyDrive/Outlook/EMResults.csv")

# Build the Calssification models and compare the results

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# Create objects of classification algorithms with default hyper-parameters

ModelLR = LogisticRegression()
ModelDC = DecisionTreeClassifier()
ModelRF = RandomForestClassifier()
ModelET = ExtraTreesClassifier()
ModelKNN = KNeighborsClassifier(n_neighbors=13)
ModelGNB = GaussianNB()
ModelSVM = SVC(kernel='rbf', random_state = 42, class_weight='balanced', probability=True)


# Evalution matrix for all the algorithms

MM = [ModelLR, ModelDC, ModelRF, ModelET, ModelKNN, ModelGNB, ModelSVM]
#MM = [ModelLR, ModelDC, ModelRF, ModelET,ModelKNN]
for models in MM:

    # Train the model training dataset

    models.fit(x_train, y_train)

    # Prediction the model with test dataset

    y_pred = models.predict(x_test)
    y_pred_prob = models.predict_proba(x_test)

    # Print the model name

    print('Model Name: ', models)

    # confusion matrix in sklearn

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import classification_report

    # actual values

    actual = y_test

    # predicted values

    predicted = y_pred

    # confusion matrix

    matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)
    print('Confusion matrix : \n', matrix)

    # outcome values order in sklearn

    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)
    print('Outcome values : \n', tp, fn, fp, tn)

    # classification report for precision, recall f1-score and accuracy

    C_Report = classification_report(actual,predicted,labels=[1,0])

    print('Classification report : \n', C_Report)

    # calculating the metrics

    sensitivity = round(tp/(tp+fn), 3);
    specificity = round(tn/(tn+fp), 3);
    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);
    balanced_accuracy = round((sensitivity+specificity)/2, 3);

    precision = round(tp/(tp+fp), 3);
    f1Score = round((2*tp/(2*tp + fp + fn)), 3);

    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1.
    # A model with a score of +1 is a perfect model and -1 is a poor model

    from math import sqrt

    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)
    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)

    print('Accuracy :', round(accuracy*100, 2),'%')
    print('Precision :', round(precision*100, 2),'%')
    print('Recall :', round(sensitivity*100,2), '%')
    print('F1 Score :', f1Score)
    print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )
    print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')
    print('MCC :', MCC)

    # Area under ROC curve

    from sklearn.metrics import roc_curve, roc_auc_score

    print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))

    # ROC Curve

    from sklearn.metrics import roc_auc_score
    from sklearn.metrics import roc_curve
    Model_roc_auc = roc_auc_score(actual, predicted)
    fpr, tpr, thresholds = roc_curve(actual, models.predict_proba(x_test)[:,1])
    plt.figure()
    #
    plt.plot(fpr, tpr, label= 'Classification Model' % Model_roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.savefig('Log_ROC')
    plt.show()
    print('-----------------------------------------------------------------------------------------------------')
    #----------------------------------------------------------------------------------------------------------
    new_row = {'Model Name' : models,
               'True_Positive': tp,
               'False_Negative': fn,
               'False_Positive': fp,
               'True_Negative': tn,
               'Accuracy' : accuracy,
               'Precision' : precision,
               'Recall' : sensitivity,
               'F1 Score' : f1Score,
               'Specificity' : specificity,
               'MCC':MCC,
               'ROC_AUC_Score':roc_auc_score(actual, predicted),
               'Balanced Accuracy':balanced_accuracy}
    EM_Result = EM_Result.append(new_row, ignore_index=True)
    #----------------------------------------------------------------------------------------------------------
#======================================================================================================================>

EM_Result

y_predF=ModelLR.predict(x_test)

Results=pd.DataFrame({'y_A':y_test, 'y_P':y_predF})
# Merge two Dataframes on index of both the dataframes
ResultsFinal=bankdata_bk.merge(Results, left_index=True, right_index=True)
# Display 5 records randomly
ResultsFinal.sample(15)

importances = ModelET.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': x.columns, 'Importance': importances})

feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
# Plot the heatmap
plt.figure(figsize=(10, 6))
plt.title("Feature Importance Heatmap")
heatmap_data = feature_importance_df.pivot(index='Feature', columns='Importance', values='Importance')
heatmap = sns.heatmap(heatmap_data, cmap="YlGnBu")
plt.show()

# Create a DataFrame to store feature names and their importances
feature_importance_df = pd.DataFrame({'Feature': x.columns, 'Importance': importances})

# Sort the features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Create a bar plot for feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.gca().invert_yaxis()  # Invert the y-axis to show the most important features at the top
plt.show()



import pandas as pd
import matplotlib.pyplot as plt

# Load your auto insurance dataset, replace 'your_dataset.csv' with the actual file path
data = pd.read_csv('/content/drive/MyDrive/Outlook/bank-direct-marketing-campaigns.csv')

# Count the number of claims and no-claims
claim_counts = data['y'].value_counts()

# Create a bar plot
plt.figure(figsize=(6, 4))
claim_counts.plot(kind='bar', color=['blue', 'green'])
plt.title('Bank Marketing Subscription')
plt.xlabel('Subscribe')
plt.ylabel('Count')
plt.xticks([0, 1], [' No Subscribe', 'Subscribe'])
plt.show()

